{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84071e3b",
   "metadata": {},
   "source": [
    "# Tutorial: Quickstart on Using the PDF Ingestion Pipeline\n",
    "\n",
    "This notebook provides a **beginner-friendly** tutorial on **how to use** a multimodal PDF ingestion pipeline. It will guide you through the **configuration** options, how to run the pipeline on your own PDFs, and what outputs to expect.\n",
    "\n",
    "We will **not** dive into the pipeline’s internal implementation code. Instead, we will focus on how to set it up and control it using the **configuration parameters** so you can quickly get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16839f7",
   "metadata": {},
   "source": [
    "## What Does This Pipeline Do?\n",
    "\n",
    "1. **Reads a PDF** and splits it into pages.\n",
    "2. **Optionally converts** pages to JPG or PNG.\n",
    "3. **Extracts text** from each page and uses a text-based LLM to process or clean that text.\n",
    "4. **Identifies embedded images** or tables using a multimodal model and generates descriptive text or Markdown.\n",
    "5. **Combines all** extracted data into a `DocumentContent` object.\n",
    "6. **Generates** doc-level outputs, such as a condensed summary and a table of contents, if requested.\n",
    "7. **Saves everything** (images, text, tables, JSON structure) into an output folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65be5f",
   "metadata": {},
   "source": [
    "## Key Configuration Flags\n",
    "\n",
    "When you create a `ProcessingPipelineConfiguration`, you can specify a **variety of flags** to tailor how the pipeline behaves:\n",
    "\n",
    "- **pdf_path**: Required string path to the PDF file to be processed.\n",
    "- **output_directory**: Optional string path to where all output files and folders will be saved.\n",
    "- **process_pages_as_jpg**: If `True`, pages are rendered as JPEG (`.jpg`); if `False`, they are rendered as PNG (`.png`).\n",
    "- **process_text**: If `True`, extracted text from each page is passed through a text-based LLM for cleanup and optional summarization.\n",
    "- **process_images**: If `True`, the pipeline searches for embedded images (photos, graphs, etc.) and uses a multimodal model to describe them.\n",
    "- **process_tables**: If `True`, the pipeline searches for table-like structures and uses a multimodal model to extract them into Markdown.\n",
    "- **save_text_files**: If `True`, a combined text-based output is saved at the doc level, along with each page’s extracted text.\n",
    "- **generate_condensed_text**: If `True`, the pipeline will create a condensed summary of the entire PDF.\n",
    "- **generate_table_of_contents**: If `True`, the pipeline will generate a table of contents from the aggregated text.\n",
    "\n",
    "### Choosing Your Models\n",
    "\n",
    "The configuration also expects **two** types of model info objects:\n",
    "\n",
    "1. **Multimodal Model Info (`MulitmodalProcessingModelInfo`)**: This model can process images, which allows the pipeline to extract insights about embedded images or tables in your PDF.\n",
    "   - `provider`: Either `\"azure\"` or `\"openai\"`.\n",
    "   - `model_name`: One of `\"gpt-4o\"` or `\"o1\"`. The pipeline requires a model that can handle images, so we **cannot** use something like `\"o1-mini\"` here.\n",
    "   - `reasoning_efforts`: Low, medium, or high. Determines how detailed or resource-intensive the reasoning might be.\n",
    "   - `endpoint`, `key`, and `api_version`: For hooking up to your cloud provider’s endpoint.\n",
    "\n",
    "2. **Text Model Info (`TextProcessingModelnfo`)**: This model is strictly for text processing, so it can be set to `\"gpt-4o\"`, `\"o1\"`, or even `\"o1-mini\"` if you want a smaller text model.\n",
    "   - `provider`: Also either `\"azure\"` or `\"openai\"`.\n",
    "   - `model_name`: Includes possible values `\"gpt-4o\"`, `\"o1\"`, or `\"o1-mini\"`.\n",
    "   - Other fields (`endpoint`, `key`, `api_version`) function similarly, specifying connection details to your text-based LLM.\n",
    "\n",
    "### Special Note on `o1-mini`\n",
    "\n",
    "Because `o1-mini` **cannot process images**, it is not suitable for the multimodal portion. That is why the code provides **two** separate classes: `MulitmodalProcessingModelInfo` for image-capable models and `TextProcessingModelnfo` for text-only models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83278fb",
   "metadata": {},
   "source": [
    "## Step-by-Step: Configuring and Running\n",
    "\n",
    "Below is an example of how to **use** the pipeline, without showing its internal code. We assume you have already installed the necessary dependencies and have the relevant classes (`ProcessingPipelineConfiguration` and `PDFIngestionPipeline`) in your codebase.\n",
    "\n",
    "We will:\n",
    "1. Import the relevant classes.\n",
    "2. Create a `ProcessingPipelineConfiguration`, explaining each parameter.\n",
    "3. Instantiate `PDFIngestionPipeline` with the config.\n",
    "4. Run the pipeline on a sample PDF.\n",
    "5. Observe the created outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bd8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the classes for configuration and pipeline usage.\n",
    "# Also import the model info classes for specifying your chosen LLM endpoints.\n",
    "# Adjust the import paths to match your project structure.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")  # Adjust the path as needed.\n",
    "\n",
    "from configuration_models import ProcessingPipelineConfiguration\n",
    "from pdf_ingestion_pipeline import PDFIngestionPipeline  \n",
    "from utils.file_utils import *\n",
    "from utils.openai_data_models import (\n",
    "    MulitmodalProcessingModelInfo,\n",
    "    TextProcessingModelnfo,\n",
    "    EmbeddingModelnfo  # In case you want an embedding model as well\n",
    ")  # adapt paths as needed\n",
    "\n",
    "# Step 2: Set up your pipeline configuration.\n",
    "# We'll pretend we have a sample PDF in 'sample_data/my_document.pdf', and we want to enable every feature.\n",
    "# We'll also choose 'gpt-4o' for both multimodal and text, but you can pick 'o1' or 'o1-mini' for text if desired.\n",
    "\n",
    "config = ProcessingPipelineConfiguration(\n",
    "    pdf_path=\"../sample_data/1_London_Brochure.pdf\",        # Required: path to your PDF.\n",
    "    output_directory=\"my_pipeline_output\",                  # Where to store the results.\n",
    "    process_pages_as_jpg=True,                    # Render PDF pages as JPEG images.\n",
    "    process_text=True,                            # Extract and process text.\n",
    "    process_images=True,                          # Analyze images.\n",
    "    process_tables=True,                          # Analyze tables.\n",
    "    save_text_files=True,                         # Save aggregated text files.\n",
    "    generate_condensed_text=True,                 # Produce a condensed summary.\n",
    "    generate_table_of_contents=True               # Generate a table of contents.\n",
    ")\n",
    "\n",
    "# Step 3: Specify your multimodal and text models.\n",
    "# Let's say you are using Azure OpenAI with an imaginary 'gpt-4o' multimodal endpoint,\n",
    "# and also an 'o1-mini' model for text. But remember, 'o1-mini' cannot handle images.\n",
    "# We show an example of how you might configure it.\n",
    "\n",
    "config.multimodal_model = MulitmodalProcessingModelInfo(\n",
    "    provider=\"azure\",                 # or 'openai'\n",
    "    model_name=\"gpt-4o\",              # can be 'gpt-4o' or 'o1' for multimodal\n",
    "    reasoning_efforts=\"high\",         # can be 'low', 'medium', or 'high' for o1 model only\n",
    "    endpoint=\"https://my-azure-endpoint.openai.azure.com/\",\n",
    "    key=\"MY_SUPER_SECRET_KEY\",\n",
    "    api_version=\"2024-12-01-preview\"   # example API version\n",
    ")\n",
    "\n",
    "config.text_model = TextProcessingModelnfo(\n",
    "    provider=\"azure\",                 # or 'openai'\n",
    "    model_name=\"o1-mini\",             # can be 'gpt-4o', 'o1', or 'o1-mini' (text only)\n",
    "    reasoning_efforts=\"medium\",\n",
    "    endpoint=\"https://my-other-azure-endpoint.openai.azure.com/\",\n",
    "    key=\"MY_OTHER_SECRET_KEY\",\n",
    "    api_version=\"2024-12-01-preview\"   # example API version\n",
    ")\n",
    "\n",
    "# Step 4: Create the pipeline and run it.\n",
    "pipeline = PDFIngestionPipeline(config)\n",
    "document_content = pipeline.process_pdf()\n",
    "\n",
    "# Step 5: Examine the 'document_content' or browse the 'my_pipeline_output' folder.\n",
    "print(\"Pipeline run complete.\")\n",
    "print(\"Number of pages processed:\", len(document_content.pages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3ab6c",
   "metadata": {},
   "source": [
    "## Understanding the Outputs\n",
    "\n",
    "1. **`my_pipeline_output/pages/page_X/`**: Contains extracted text, images, tables, and combined data for each page.\n",
    "2. **`my_pipeline_output/text_twin.md`**: All pages’ text combined into a single Markdown file.\n",
    "3. **`my_pipeline_output/condensed_text.md`**: A short summary of the PDF, if you turned on `generate_condensed_text`.\n",
    "4. **`my_pipeline_output/table_of_contents.md`**: A table of contents, if you enabled `generate_table_of_contents`.\n",
    "5. **`my_pipeline_output/document_content.json`**: A complete JSON representation of the entire document, including all pages and assets.\n",
    "\n",
    "At this point, you have a variety of rich outputs describing the contents of your PDF. You can stop here or use them in subsequent steps for additional analytics, indexing, or other custom pipelines.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By following this tutorial, you can easily set up your pipeline with **fine-grained control** over how it processes PDFs (images, tables, text extraction, and so on). You can also swap in different multimodal and text models to tailor performance and cost to your needs. In addition, you can see exactly what is generated in your output folder and continue your workflow from there.\n",
    "\n",
    "Good luck exploring the **multimodal** capabilities of LLMs with your own PDF documents!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightmmrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
